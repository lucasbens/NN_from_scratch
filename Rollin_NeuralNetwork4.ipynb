{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StepFunction:\n",
    "    @staticmethod\n",
    "    def apply(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def apply(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def apply_(x):\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def apply(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def apply_(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "class Softmax:\n",
    "    @staticmethod\n",
    "    def apply(x):\n",
    "        return np.exp(x) / np.exp(x).sum()\n",
    "        \n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def loss_(y_pred, y_true):\n",
    "        return 2 * (y_true - y_pred)\n",
    "        \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, weights,  biases=None, activation_func=Sigmoid):\n",
    "    # you can choose whether to add bias manually or use padding as we saw in class\n",
    "        self.weights = weights\n",
    "        self.activation_func = activation_func\n",
    "        if biases is None:\n",
    "            self.biases = np.zeros(self.weights.shape[0])\n",
    "        else:\n",
    "            self.biases = biases\n",
    "        \n",
    "    \n",
    "    def apply(self, x_in):\n",
    "    # calculate the layer output on the given x_in using layer weights and biases\n",
    "        output = np.dot(self.weights, x_in) + self.biases\n",
    "        self.x_out = output\n",
    "        self.x_in = x_in\n",
    "        return self.activation_func.apply(output)\n",
    "    \n",
    "    def backwards(self, downstream_grad):\n",
    "    # calculate the gradient of the layer weights and pad on the gradient to upstream layers\n",
    "    # this implementation assumes biases are padded to the weights matrix\n",
    "        x_out_grad = self.activation_func.apply_(self.x_out)\n",
    "        downstream_grad = downstream_grad * x_out_grad\n",
    "        self.weights_grad = np.outer(downstream_grad, self.x_in)\n",
    "        self.bias_grad = downstream_grad\n",
    "        downstream_grad = downstream_grad @ self.weights\n",
    "        return downstream_grad\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers=None, loss_func=MSE):\n",
    "    # initialize the network with the given layers\n",
    "        if layers is None:\n",
    "            self.layers = []\n",
    "        else:\n",
    "            self.layers = layers\n",
    "        self.check_shapes()\n",
    "\n",
    "    def add_layer(self, new_layer, pos = None):\n",
    "    # add a layer to the network\n",
    "    # bonus - do not assume valid input, check the new layer shape matches the existing layers\n",
    "        if pos is None:\n",
    "            self.layers.insert(len(self.layers), new_layer)\n",
    "        else:\n",
    "            self.layers.insert(pos, new_layer)\n",
    "        self.check_shapes()\n",
    "\n",
    "    def remove_layer(self, pos = None):\n",
    "        if pos is None:\n",
    "            pass\n",
    "        else:\n",
    "            del self.layers[pos]\n",
    "        self.check_shapes()\n",
    "    \n",
    "    def check_shapes(self):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            assert self.layers[i-1].weights.shape[0] == self.layers[i].weights.shape[1]\n",
    "            \n",
    "    def forward(self, x):\n",
    "    # do a forward pass of the network on the given input\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                res = layer.apply(x)\n",
    "            else:\n",
    "                res = layer.apply(res)\n",
    "        return res\n",
    "    \n",
    "    def backwards(self, loss_grad):\n",
    "    # do a backwards pass and calculate the network gradients\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backwards(loss_grad)\n",
    "        return loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR loss is: 10.03\n",
      "XOR accuracy is: 0.0\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.normal(3, 1, size=(2,2))\n",
    "b1 = np.array([0., -1.])\n",
    "x1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "layer1 = Layer(w1, b1, activation_func=Sigmoid)\n",
    "w2 = np.random.normal(3, 1, size=(1,2))\n",
    "layer2 = Layer(w2, activation_func=ReLU)\n",
    "\n",
    "y_true = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "net = Network([layer1, layer2])\n",
    "\n",
    "total_loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "for i, item in enumerate(x1):\n",
    "    y_pred_prob = net.forward(item)\n",
    "    loss = MSE.loss(y_true[i], y_pred_prob)\n",
    "    total_loss += loss / x1.shape[0]\n",
    "    \n",
    "    if y_pred_prob == y_true[i]:\n",
    "        acc = 1\n",
    "    else:\n",
    "        acc = 0    \n",
    "    \n",
    "    accuracy += acc / len(y_true)\n",
    "    \n",
    "print('XOR loss is:', round(total_loss, 2))\n",
    "print('XOR accuracy is:', round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XOR Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 has a total loss of: 3.216951415935082\n",
      "Iteration 0 has a total accuracy of: 0.0\n",
      "Iteration 100 has a total loss of: 0.5173908018387787\n",
      "Iteration 100 has a total accuracy of: 0.25\n",
      "Iteration 200 has a total loss of: 0.5123842571077227\n",
      "Iteration 200 has a total accuracy of: 0.25\n",
      "Iteration 300 has a total loss of: 0.5101689330266611\n",
      "Iteration 300 has a total accuracy of: 0.25\n",
      "Iteration 400 has a total loss of: 0.34505711048560206\n",
      "Iteration 400 has a total accuracy of: 0.0\n",
      "Iteration 500 has a total loss of: 0.28296525837462294\n",
      "Iteration 500 has a total accuracy of: 0.0\n",
      "Iteration 600 has a total loss of: 0.248014174257486\n",
      "Iteration 600 has a total accuracy of: 0.0\n",
      "Iteration 700 has a total loss of: 0.20267058222638143\n",
      "Iteration 700 has a total accuracy of: 0.0\n",
      "Iteration 800 has a total loss of: 0.00047028331314161414\n",
      "Iteration 800 has a total accuracy of: 0.0\n",
      "Iteration 900 has a total loss of: 1.886641933609764e-07\n",
      "Iteration 900 has a total accuracy of: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(len(y_true)):\n",
    "        y_t = y_true[j]\n",
    "        x_t = x1[j]\n",
    "        \n",
    "        y_pred = net.forward(x_t)\n",
    "        loss = MSE.loss(y_t, y_pred)\n",
    "        loss_ = MSE.loss_(y_t, y_pred)\n",
    "        \n",
    "        total_loss += loss / x1.shape[0]\n",
    "        \n",
    "        net.backwards(loss_)\n",
    "        \n",
    "        for layer in net.layers:\n",
    "            layer.weights -= layer.weights_grad * lr\n",
    "            layer.biases -= layer.bias_grad * lr\n",
    "                \n",
    "        if round(y_pred[0], 10) == round(y_t[0], 10):\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0    \n",
    "    \n",
    "        accuracy += acc / len(y_true)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration', i, 'has a total loss of:', total_loss)\n",
    "        print('Iteration', i, 'has a total accuracy of:', accuracy)\n",
    "    total_loss = 0\n",
    "    accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will increase the number of epochs to see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 has a total loss of: 5.204799090195619\n",
      "Iteration 0 has a total accuracy of: 0.0\n",
      "Iteration 100 has a total loss of: 0.2697454723685644\n",
      "Iteration 100 has a total accuracy of: 0.0\n",
      "Iteration 200 has a total loss of: 0.0003840129233990444\n",
      "Iteration 200 has a total accuracy of: 0.0\n",
      "Iteration 300 has a total loss of: 2.6595069193625025e-08\n",
      "Iteration 300 has a total accuracy of: 0.0\n",
      "Iteration 400 has a total loss of: 6.422005899346851e-13\n",
      "Iteration 400 has a total accuracy of: 0.25\n",
      "Iteration 500 has a total loss of: 7.159480107396371e-18\n",
      "Iteration 500 has a total accuracy of: 0.25\n",
      "Iteration 600 has a total loss of: 7.87596686841585e-23\n",
      "Iteration 600 has a total accuracy of: 1.0\n",
      "Iteration 700 has a total loss of: 8.714078033814042e-28\n",
      "Iteration 700 has a total accuracy of: 1.0\n",
      "Iteration 800 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 800 has a total accuracy of: 1.0\n",
      "Iteration 900 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 900 has a total accuracy of: 1.0\n",
      "Iteration 1000 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1000 has a total accuracy of: 1.0\n",
      "Iteration 1100 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1100 has a total accuracy of: 1.0\n",
      "Iteration 1200 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1200 has a total accuracy of: 1.0\n",
      "Iteration 1300 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1300 has a total accuracy of: 1.0\n",
      "Iteration 1400 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1400 has a total accuracy of: 1.0\n",
      "Iteration 1500 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1500 has a total accuracy of: 1.0\n",
      "Iteration 1600 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1600 has a total accuracy of: 1.0\n",
      "Iteration 1700 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1700 has a total accuracy of: 1.0\n",
      "Iteration 1800 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1800 has a total accuracy of: 1.0\n",
      "Iteration 1900 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 1900 has a total accuracy of: 1.0\n",
      "Iteration 2000 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2000 has a total accuracy of: 1.0\n",
      "Iteration 2100 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2100 has a total accuracy of: 1.0\n",
      "Iteration 2200 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2200 has a total accuracy of: 1.0\n",
      "Iteration 2300 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2300 has a total accuracy of: 1.0\n",
      "Iteration 2400 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2400 has a total accuracy of: 1.0\n",
      "Iteration 2500 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2500 has a total accuracy of: 1.0\n",
      "Iteration 2600 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2600 has a total accuracy of: 1.0\n",
      "Iteration 2700 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2700 has a total accuracy of: 1.0\n",
      "Iteration 2800 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2800 has a total accuracy of: 1.0\n",
      "Iteration 2900 has a total loss of: 2.4775162804597402e-30\n",
      "Iteration 2900 has a total accuracy of: 1.0\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.normal(3, 1, size=(2,2))\n",
    "b1 = np.array([0., -1.])\n",
    "x1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "layer1 = Layer(w1, b1, activation_func=Sigmoid)\n",
    "w2 = np.random.normal(3, 1, size=(1,2))\n",
    "layer2 = Layer(w2, activation_func=ReLU)\n",
    "\n",
    "y_true = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "net = Network([layer1, layer2])\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(len(y_true)):\n",
    "        y_t = y_true[j]\n",
    "        x_t = x1[j]\n",
    "        \n",
    "        y_pred = net.forward(x_t)\n",
    "        loss = MSE.loss(y_t, y_pred)\n",
    "        loss_ = MSE.loss_(y_t, y_pred)\n",
    "        \n",
    "        total_loss += loss / x1.shape[0]\n",
    "        \n",
    "        net.backwards(loss_)\n",
    "        \n",
    "        for layer in net.layers:\n",
    "            layer.weights -= layer.weights_grad * lr\n",
    "            layer.biases -= layer.bias_grad * lr\n",
    "                \n",
    "        if round(y_pred[0], 10) == round(y_t[0], 10):\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0    \n",
    "    \n",
    "        accuracy += acc / len(y_true)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration', i, 'has a total loss of:', total_loss)\n",
    "        print('Iteration', i, 'has a total accuracy of:', accuracy)\n",
    "    total_loss = 0\n",
    "    accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that compared to the first 1000 epochs this one converges to the desired accuracy. Also, with less epochs this could have happened, basically depends on the initialization weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 has a total loss of: 6.497271881905371\n",
      "Iteration 0 has a total accuracy of: 0.0\n",
      "Iteration 100 has a total loss of: 0.2541368934328439\n",
      "Iteration 100 has a total accuracy of: 0.25\n",
      "Iteration 200 has a total loss of: 0.23233098090241414\n",
      "Iteration 200 has a total accuracy of: 0.25\n",
      "Iteration 300 has a total loss of: 0.22002860258287538\n",
      "Iteration 300 has a total accuracy of: 0.25\n",
      "Iteration 400 has a total loss of: 0.2126586290442939\n",
      "Iteration 400 has a total accuracy of: 0.0\n",
      "Iteration 500 has a total loss of: 0.2083707490469045\n",
      "Iteration 500 has a total accuracy of: 0.0\n",
      "Iteration 600 has a total loss of: 0.2053428025455065\n",
      "Iteration 600 has a total accuracy of: 0.0\n",
      "Iteration 700 has a total loss of: 0.20298726779410428\n",
      "Iteration 700 has a total accuracy of: 0.0\n",
      "Iteration 800 has a total loss of: 0.2010687746586444\n",
      "Iteration 800 has a total accuracy of: 0.0\n",
      "Iteration 900 has a total loss of: 0.199465569041607\n",
      "Iteration 900 has a total accuracy of: 0.0\n",
      "Iteration 1000 has a total loss of: 0.1981017971196087\n",
      "Iteration 1000 has a total accuracy of: 0.0\n",
      "Iteration 1100 has a total loss of: 0.196925018322843\n",
      "Iteration 1100 has a total accuracy of: 0.0\n",
      "Iteration 1200 has a total loss of: 0.19589703107149659\n",
      "Iteration 1200 has a total accuracy of: 0.0\n",
      "Iteration 1300 has a total loss of: 0.1949892194763193\n",
      "Iteration 1300 has a total accuracy of: 0.0\n",
      "Iteration 1400 has a total loss of: 0.19417976354733402\n",
      "Iteration 1400 has a total accuracy of: 0.0\n",
      "Iteration 1500 has a total loss of: 0.1934517939611415\n",
      "Iteration 1500 has a total accuracy of: 0.0\n",
      "Iteration 1600 has a total loss of: 0.19279210856878962\n",
      "Iteration 1600 has a total accuracy of: 0.0\n",
      "Iteration 1700 has a total loss of: 0.19219025605294754\n",
      "Iteration 1700 has a total accuracy of: 0.0\n",
      "Iteration 1800 has a total loss of: 0.1916378714236352\n",
      "Iteration 1800 has a total accuracy of: 0.0\n",
      "Iteration 1900 has a total loss of: 0.19112818856919167\n",
      "Iteration 1900 has a total accuracy of: 0.0\n",
      "Iteration 2000 has a total loss of: 0.19065567909308045\n",
      "Iteration 2000 has a total accuracy of: 0.0\n",
      "Iteration 2100 has a total loss of: 0.19021578217073626\n",
      "Iteration 2100 has a total accuracy of: 0.0\n",
      "Iteration 2200 has a total loss of: 0.18980470060705862\n",
      "Iteration 2200 has a total accuracy of: 0.0\n",
      "Iteration 2300 has a total loss of: 0.189419245462633\n",
      "Iteration 2300 has a total accuracy of: 0.0\n",
      "Iteration 2400 has a total loss of: 0.18905671661979592\n",
      "Iteration 2400 has a total accuracy of: 0.0\n",
      "Iteration 2500 has a total loss of: 0.18871481017102415\n",
      "Iteration 2500 has a total accuracy of: 0.0\n",
      "Iteration 2600 has a total loss of: 0.18839154599456942\n",
      "Iteration 2600 has a total accuracy of: 0.0\n",
      "Iteration 2700 has a total loss of: 0.1880852106500213\n",
      "Iteration 2700 has a total accuracy of: 0.0\n",
      "Iteration 2800 has a total loss of: 0.18779431199458502\n",
      "Iteration 2800 has a total accuracy of: 0.0\n",
      "Iteration 2900 has a total loss of: 0.18751754283743322\n",
      "Iteration 2900 has a total accuracy of: 0.0\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.normal(3, 1, size=(2,2))\n",
    "b1 = np.array([0., -1.])\n",
    "x1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "layer1 = Layer(w1, b1, activation_func=Sigmoid)\n",
    "w2 = np.random.normal(3, 1, size=(1,2))\n",
    "layer2 = Layer(w2, activation_func=ReLU)\n",
    "\n",
    "y_true = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "net = Network([layer1, layer2])\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(len(y_true)):\n",
    "        y_t = y_true[j]\n",
    "        x_t = x1[j]\n",
    "        \n",
    "        y_pred = net.forward(x_t)\n",
    "        loss = MSE.loss(y_t, y_pred)\n",
    "        loss_ = MSE.loss_(y_t, y_pred)\n",
    "        \n",
    "        total_loss += loss / x1.shape[0]\n",
    "        \n",
    "        net.backwards(loss_)\n",
    "        \n",
    "        for layer in net.layers:\n",
    "            layer.weights -= layer.weights_grad * lr\n",
    "            layer.biases -= layer.bias_grad * lr\n",
    "                \n",
    "        if round(y_pred[0], 10) == round(y_t[0], 10):\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0    \n",
    "    \n",
    "        accuracy += acc / len(y_true)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration', i, 'has a total loss of:', total_loss)\n",
    "        print('Iteration', i, 'has a total accuracy of:', accuracy)\n",
    "    total_loss = 0\n",
    "    accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller learning rate doesn't make it improve a lot... It doesn't let it 'escape'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 has a total loss of: 5.117593580408551\n",
      "Iteration 0 has a total accuracy of: 0.25\n",
      "Iteration 100 has a total loss of: 0.5\n",
      "Iteration 100 has a total accuracy of: 0.5\n",
      "Iteration 200 has a total loss of: 0.5\n",
      "Iteration 200 has a total accuracy of: 0.5\n",
      "Iteration 300 has a total loss of: 0.5\n",
      "Iteration 300 has a total accuracy of: 0.5\n",
      "Iteration 400 has a total loss of: 0.5\n",
      "Iteration 400 has a total accuracy of: 0.5\n",
      "Iteration 500 has a total loss of: 0.5\n",
      "Iteration 500 has a total accuracy of: 0.5\n",
      "Iteration 600 has a total loss of: 0.5\n",
      "Iteration 600 has a total accuracy of: 0.5\n",
      "Iteration 700 has a total loss of: 0.5\n",
      "Iteration 700 has a total accuracy of: 0.5\n",
      "Iteration 800 has a total loss of: 0.5\n",
      "Iteration 800 has a total accuracy of: 0.5\n",
      "Iteration 900 has a total loss of: 0.5\n",
      "Iteration 900 has a total accuracy of: 0.5\n",
      "Iteration 1000 has a total loss of: 0.5\n",
      "Iteration 1000 has a total accuracy of: 0.5\n",
      "Iteration 1100 has a total loss of: 0.5\n",
      "Iteration 1100 has a total accuracy of: 0.5\n",
      "Iteration 1200 has a total loss of: 0.5\n",
      "Iteration 1200 has a total accuracy of: 0.5\n",
      "Iteration 1300 has a total loss of: 0.5\n",
      "Iteration 1300 has a total accuracy of: 0.5\n",
      "Iteration 1400 has a total loss of: 0.5\n",
      "Iteration 1400 has a total accuracy of: 0.5\n",
      "Iteration 1500 has a total loss of: 0.5\n",
      "Iteration 1500 has a total accuracy of: 0.5\n",
      "Iteration 1600 has a total loss of: 0.5\n",
      "Iteration 1600 has a total accuracy of: 0.5\n",
      "Iteration 1700 has a total loss of: 0.5\n",
      "Iteration 1700 has a total accuracy of: 0.5\n",
      "Iteration 1800 has a total loss of: 0.5\n",
      "Iteration 1800 has a total accuracy of: 0.5\n",
      "Iteration 1900 has a total loss of: 0.5\n",
      "Iteration 1900 has a total accuracy of: 0.5\n",
      "Iteration 2000 has a total loss of: 0.5\n",
      "Iteration 2000 has a total accuracy of: 0.5\n",
      "Iteration 2100 has a total loss of: 0.5\n",
      "Iteration 2100 has a total accuracy of: 0.5\n",
      "Iteration 2200 has a total loss of: 0.5\n",
      "Iteration 2200 has a total accuracy of: 0.5\n",
      "Iteration 2300 has a total loss of: 0.5\n",
      "Iteration 2300 has a total accuracy of: 0.5\n",
      "Iteration 2400 has a total loss of: 0.5\n",
      "Iteration 2400 has a total accuracy of: 0.5\n",
      "Iteration 2500 has a total loss of: 0.5\n",
      "Iteration 2500 has a total accuracy of: 0.5\n",
      "Iteration 2600 has a total loss of: 0.5\n",
      "Iteration 2600 has a total accuracy of: 0.5\n",
      "Iteration 2700 has a total loss of: 0.5\n",
      "Iteration 2700 has a total accuracy of: 0.5\n",
      "Iteration 2800 has a total loss of: 0.5\n",
      "Iteration 2800 has a total accuracy of: 0.5\n",
      "Iteration 2900 has a total loss of: 0.5\n",
      "Iteration 2900 has a total accuracy of: 0.5\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.normal(3, 1, size=(2,2))\n",
    "b1 = np.array([0., -1.])\n",
    "x1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "layer1 = Layer(w1, b1, activation_func=Sigmoid)\n",
    "w2 = np.random.normal(3, 1, size=(1,2))\n",
    "layer2 = Layer(w2, activation_func=ReLU)\n",
    "\n",
    "y_true = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "net = Network([layer1, layer2])\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "lr = 0.4\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(len(y_true)):\n",
    "        y_t = y_true[j]\n",
    "        x_t = x1[j]\n",
    "        \n",
    "        y_pred = net.forward(x_t)\n",
    "        loss = MSE.loss(y_t, y_pred)\n",
    "        loss_ = MSE.loss_(y_t, y_pred)\n",
    "        \n",
    "        total_loss += loss / x1.shape[0]\n",
    "        \n",
    "        net.backwards(loss_)\n",
    "        \n",
    "        for layer in net.layers:\n",
    "            layer.weights -= layer.weights_grad * lr\n",
    "            layer.biases -= layer.bias_grad * lr\n",
    "                \n",
    "        if round(y_pred[0], 10) == round(y_t[0], 10):\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0    \n",
    "    \n",
    "        accuracy += acc / len(y_true)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration', i, 'has a total loss of:', total_loss)\n",
    "        print('Iteration', i, 'has a total accuracy of:', accuracy)\n",
    "    total_loss = 0\n",
    "    accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a learning rate to high neither... It's kind of 'jumping' a lot between values in the function and not learning anything that's why we get basically the average of guessing if it was going to be a 1 or a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with mnist__weights.npz ........... \n",
      "\n",
      "Iteration 0 has a total loss of: 0.007209368311541007\n",
      "Iteration 0 has a total accuracy of: 0.9820000000000008\n",
      "Iteration 10 has a total loss of: 0.0008513732864818982\n",
      "Iteration 10 has a total accuracy of: 0.9960000000000008\n",
      "Iteration 20 has a total loss of: 0.0004006761755174906\n",
      "Iteration 20 has a total accuracy of: 0.9980000000000008\n",
      "Iteration 30 has a total loss of: 0.00031838605855695186\n",
      "Iteration 30 has a total accuracy of: 0.9980000000000008\n",
      "Iteration 40 has a total loss of: 0.0002834493680628709\n",
      "Iteration 40 has a total accuracy of: 0.9980000000000008\n",
      "\n",
      "Working with Random Shifted ........... \n",
      "\n",
      "Iteration 0 has a total loss of: 0.03865353767544305\n",
      "Iteration 0 has a total accuracy of: 0.8090000000000006\n",
      "Iteration 10 has a total loss of: 0.0024124655344902085\n",
      "Iteration 10 has a total accuracy of: 0.9860000000000008\n",
      "Iteration 20 has a total loss of: 0.001496247598081146\n",
      "Iteration 20 has a total accuracy of: 0.9880000000000008\n",
      "Iteration 30 has a total loss of: 0.0011480808645449024\n",
      "Iteration 30 has a total accuracy of: 0.9910000000000008\n",
      "Iteration 40 has a total loss of: 0.0010211623086927024\n",
      "Iteration 40 has a total accuracy of: 0.9920000000000008\n",
      "\n",
      "Working with Random ........... \n",
      "\n",
      "Iteration 0 has a total loss of: 0.07693794556442267\n",
      "Iteration 0 has a total accuracy of: 0.6570000000000005\n",
      "Iteration 10 has a total loss of: 0.0033889404365096253\n",
      "Iteration 10 has a total accuracy of: 0.9770000000000008\n",
      "Iteration 20 has a total loss of: 0.0020356927868399825\n",
      "Iteration 20 has a total accuracy of: 0.9870000000000008\n",
      "Iteration 30 has a total loss of: 0.001524881588104903\n",
      "Iteration 30 has a total accuracy of: 0.9870000000000008\n",
      "Iteration 40 has a total loss of: 0.0011988534715004467\n",
      "Iteration 40 has a total accuracy of: 0.9910000000000008\n"
     ]
    }
   ],
   "source": [
    "targets = ['mnist__weights.npz', 'Random Shifted', 'Random']\n",
    "\n",
    "npzfile = np.load('mnist__weights.npz')\n",
    "\n",
    "x = npzfile['x']\n",
    "y = npzfile['y']\n",
    "\n",
    "for target in targets:\n",
    "    \n",
    "    print('\\nWorking with', target, '........... \\n')\n",
    "\n",
    "    if target == 'mnist__weights.npz':\n",
    "        \n",
    "        b1 = npzfile['l1_bias']\n",
    "        b2 = npzfile['l2_bias']\n",
    "        b3 = npzfile['l3_bias']\n",
    "        \n",
    "        w1 = npzfile['l1_weights']\n",
    "        w2 = npzfile['l2_weights']\n",
    "        w3 = npzfile['l3_weights']\n",
    "    \n",
    "    elif target == 'Random Shifted':\n",
    "        \n",
    "        b1 = npzfile['l1_bias']\n",
    "        b2 = npzfile['l2_bias']\n",
    "        b3 = npzfile['l3_bias']\n",
    "        \n",
    "        w1 = npzfile['l1_weights'] + 2 * np.random.random_sample((b1.shape[0], x.shape[1])) - 1\n",
    "        w2 = npzfile['l2_weights'] + 2 * np.random.random_sample((b2.shape[0], b1.shape[0])) - 1\n",
    "        w3 = npzfile['l3_weights'] + 2 * np.random.random_sample((b3.shape[0], b2.shape[0])) - 1\n",
    "    \n",
    "    elif target == 'Random':\n",
    "        \n",
    "        b1 = np.random.normal(3, 1, size=(128,))\n",
    "        b2 = np.random.normal(3, 1, size=(64,))\n",
    "        b3 = np.random.normal(3, 1, size=(10,))\n",
    "        \n",
    "        w1 = npzfile['l1_weights'] + 2 * np.random.random_sample((b1.shape[0], x.shape[1])) - 1\n",
    "        w2 = npzfile['l2_weights'] + 2 * np.random.random_sample((b2.shape[0], b1.shape[0])) - 1\n",
    "        w3 = npzfile['l3_weights'] + 2 * np.random.random_sample((b3.shape[0], b2.shape[0])) - 1\n",
    "    \n",
    "    epochs = 50\n",
    "\n",
    "    lr = 0.1\n",
    "    \n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    layer1 = Layer(w1, b1, activation_func=Sigmoid)\n",
    "    layer2 = Layer(w2, b2, activation_func=Sigmoid)\n",
    "    layer3 = Layer(w3, b3, activation_func=Sigmoid)\n",
    "\n",
    "    net = Network([layer1, layer2, layer3])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        for j in range(x.shape[0]):\n",
    "            \n",
    "            obs = j\n",
    "            y_pred_prob = np.array(net.forward(x[obs]))\n",
    "            y_true = np.array([1 if y[obs] == i else 0 for i in range(y_pred_prob.shape[0])])\n",
    "            \n",
    "            net.backwards(MSE.loss_(y_true, y_pred_prob))\n",
    "            \n",
    "            for layer in net.layers:\n",
    "                layer.weights -= layer.weights_grad * lr\n",
    "                layer.biases -= layer.bias_grad * lr\n",
    "            \n",
    "            y_pred = y_pred_prob.argmax()\n",
    "            y_true_pos = y_true.argmax()\n",
    "    \n",
    "            if y_pred == y_true_pos:\n",
    "                acc = 1\n",
    "            else:\n",
    "                acc = 0\n",
    "        \n",
    "            accuracy += acc / x.shape[0]\n",
    "    \n",
    "            total_loss += MSE.loss(y_true, y_pred_prob) / x.shape[0]\n",
    "\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            print('Iteration', i, 'has a total loss of:', total_loss)\n",
    "            print('Iteration', i, 'has a total accuracy of:', accuracy)\n",
    "        total_loss = 0\n",
    "        accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! we can clearly see that the pre trained weights gave the best accuracy after 50 epochs of 99.8%!. In the overall we can see how the model improves while adjusting the weights and biases based on the gradients!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
